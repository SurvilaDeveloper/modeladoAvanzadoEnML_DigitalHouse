{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2438882a",
   "metadata": {},
   "source": [
    "## Normalizaci√≥n\n",
    "\n",
    "Cuando se normalizan las features de un DataFrame con StandardScaler de scikit-learn, cada feature se transforma de forma independiente usando su propio factor de escala (desviaci√≥n est√°ndar) y su propio valor de desplazamiento (media).\n",
    "\n",
    "### ¬øC√≥mo funciona StandardScaler?\n",
    "\n",
    "StandardScaler transforma cada feature $ x_i $ de la siguiente manera:\n",
    "\n",
    "$$\n",
    "x_i^{\\text{scaled}} = \\frac{x_i - \\mu_i}{\\sigma_i}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $ \\mu_i $ es la **media** de la feature $ i $, calculada como:\n",
    "\n",
    "  $$\n",
    "  \\mu_i = \\frac{1}{n} \\sum_{j=1}^{n} x_{ij}\n",
    "  $$\n",
    "\n",
    "- $ \\sigma_i $ es la **desviaci√≥n est√°ndar** de la feature $ i $, calculada como:\n",
    "\n",
    "  $$\n",
    "  \\sigma_i = \\sqrt{\\frac{1}{n} \\sum_{j=1}^{n} (x_{ij} - \\mu_i)^2}\n",
    "  $$\n",
    "\n",
    "Aqu√≠:\n",
    "\n",
    "- $ x_{ij} $ es el valor de la feature $ i $ para el ejemplo $ j $\n",
    "- $ n $ es el n√∫mero total de muestras\n",
    "\n",
    "\n",
    "Cada feature se escala (y desplaza) con sus propios valores de media y desviaci√≥n est√°ndar.\n",
    "Esto garantiza que cada columna del DataFrame resultante tenga media 0 y desviaci√≥n est√°ndar 1, permitiendo comparaciones m√°s justas entre features con escalas diferentes.\n",
    "\n",
    "No se ajusta el modelo para que funcione con las features sin normalizar.\n",
    "En su lugar, se normalizan tambi√©n las features nuevas (de entrada) antes de usarlas con el modelo entrenado.\n",
    "\n",
    "### ¬øPor qu√© no se \"des-normaliza\" el modelo?\n",
    "Cuando entren√°s un modelo con features normalizadas (por ejemplo, usando `StandardScaler`), los par√°metros internos del modelo se ajustan a esas escalas. Cambiar las escalas de las features despu√©s sin volver a entrenar el modelo afectar√≠a directamente su desempe√±o y precisi√≥n.\n",
    "\n",
    "### Flujo correcto:\n",
    "\n",
    "1. Durante el entrenamiento\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "scaler = StandardScaler()\n",
    "model = LinearRegression()\n",
    "\n",
    "# Encapsular todo en un pipeline\n",
    "pipeline = make_pipeline(scaler, model)\n",
    "\n",
    "# Entrenar\n",
    "pipeline.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "2. Durante la predicci√≥n (en producci√≥n):\n",
    "\n",
    "```python\n",
    "# Las nuevas features sin normalizar\n",
    "X_new = ...\n",
    "\n",
    "# El pipeline ya sabe c√≥mo escalar porque guarda los par√°metros del scaler\n",
    "y_pred = pipeline.predict(X_new)\n",
    "```\n",
    "\n",
    "**‚úÖ El StandardScaler guarda internamente la media y desviaci√≥n est√°ndar, y se usa para transformar los datos nuevos de forma coherente.**\n",
    "\n",
    "**¬øY si ya ten√©s el modelo entrenado pero quer√©s hacerlo funcionar con features sin escalar?**  \n",
    "Tendr√≠as que reentrenar el modelo directamente con los datos sin escalar, o hacer ingenier√≠a inversa para \"desescalar\" los coeficientes del modelo, lo cual es complicado y propenso a errores (y solo posible en modelos lineales simples).\n",
    "\n",
    "### Conclusi√≥n\n",
    "* Nunca \"desnormalices\" el modelo.\n",
    "\n",
    "* Siempre normaliz√° las nuevas features igual que las del entrenamiento.\n",
    "\n",
    "* Us√° un `Pipeline` de `scikit-learn` para que esto sea autom√°tico y robusto.\n",
    "\n",
    "## C√≥mo **guardar** y **cargar** un `Pipeline` con `StandardScaler` y un modelo, usando `joblib` (recomendado por `scikit-learn` para este tipo de tareas).\n",
    "\n",
    "### ‚úÖ Paso 1: Entrenar, crear y guardar el pipeline\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import joblib\n",
    "\n",
    "# Entrenamiento\n",
    "scaler = StandardScaler()\n",
    "model = LinearRegression()\n",
    "\n",
    "# Crear pipeline\n",
    "pipeline = make_pipeline(scaler, model)\n",
    "\n",
    "# Ajustar con los datos de entrenamiento\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Guardar el pipeline en un archivo\n",
    "joblib.dump(pipeline, 'modelo_entrenado.pkl')\n",
    "\n",
    "```\n",
    "\n",
    "### ‚úÖ Paso 2: Cargar el pipeline entrenado y hacer predicciones\n",
    "\n",
    "```python\n",
    "# Cargar el pipeline entrenado desde el archivo\n",
    "pipeline_cargado = joblib.load('modelo_entrenado.pkl')\n",
    "\n",
    "# Nuevos datos sin escalar\n",
    "X_nuevo = ...\n",
    "\n",
    "# Predecir directamente (el pipeline aplica el StandardScaler internamente)\n",
    "y_pred = pipeline_cargado.predict(X_nuevo)\n",
    "\n",
    "```\n",
    "\n",
    "### üîê ¬øPor qu√© esto es √∫til?\n",
    "* No necesit√°s preocuparte por guardar `mean_` y `scale_` del scaler.\n",
    "\n",
    "* Todo est√° encapsulado: el escalado y el modelo.\n",
    "\n",
    "* Es ideal para producci√≥n o integraci√≥n con APIs.\n",
    "\n",
    "## Aqu√≠ va un ejemplo completo y funcional con datos ficticios:\n",
    "\n",
    "### üß™ Ejemplo completo: `StandardScaler` + `LinearRegression` + Pipeline  \n",
    "### üì¶ Librer√≠as necesarias\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import joblib\n",
    "\n",
    "```\n",
    "\n",
    "### üß∑ 1. Generar datos ficticios\n",
    "\n",
    "```python\n",
    "# Datos: 100 samples, 2 features\n",
    "X = np.random.rand(100, 2) * 100  # Valores entre 0 y 100\n",
    "y = 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(100) * 10  # y con algo de ruido\n",
    "\n",
    "# Separar en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "```\n",
    "\n",
    "### ‚öôÔ∏è 2. Crear, entrenar y guardar el pipeline\n",
    "\n",
    "```python\n",
    "# Crear pipeline con StandardScaler y regresi√≥n lineal\n",
    "pipeline = make_pipeline(StandardScaler(), LinearRegression())\n",
    "\n",
    "# Entrenar el pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Guardar el pipeline en un archivo\n",
    "joblib.dump(pipeline, 'modelo_escalado.pkl')\n",
    "\n",
    "```\n",
    "\n",
    "### üß† 3. Cargar y usar el modelo en producci√≥n\n",
    "\n",
    "```python\n",
    "# Cargar el pipeline entrenado\n",
    "modelo_cargado = joblib.load('modelo_escalado.pkl')\n",
    "\n",
    "# Simular nuevos datos crudos (sin escalar)\n",
    "X_nuevos = np.array([[10, 20], [50, 60]])\n",
    "\n",
    "# Predecir\n",
    "predicciones = modelo_cargado.predict(X_nuevos)\n",
    "\n",
    "print(\"Predicciones:\", predicciones)\n",
    "\n",
    "```\n",
    "\n",
    "### ‚úÖ Salida esperada\n",
    "Vas a ver las predicciones que el modelo entrenado genera para los datos `[10, 20]` y `[50, 60]`, aplicando internamente el `StandardScaler` con los par√°metros aprendidos durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "515ee017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones: [ 64.80315401 268.28548109]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import joblib\n",
    "\n",
    "# Datos: 100 samples, 2 features\n",
    "X = np.random.rand(100, 2) * 100  # Valores entre 0 y 100\n",
    "y = 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(100) * 10  # y con algo de ruido\n",
    "\n",
    "# Separar en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear pipeline con StandardScaler y regresi√≥n lineal\n",
    "pipeline = make_pipeline(StandardScaler(), LinearRegression())\n",
    "\n",
    "# Entrenar el pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Guardar el pipeline en un archivo\n",
    "joblib.dump(pipeline, 'modelo_escalado.pkl')\n",
    "\n",
    "# Cargar el pipeline entrenado\n",
    "modelo_cargado = joblib.load('modelo_escalado.pkl')\n",
    "\n",
    "# Simular nuevos datos crudos (sin escalar)\n",
    "X_nuevos = np.array([[10, 20], [50, 60]])\n",
    "\n",
    "# Predecir\n",
    "predicciones = modelo_cargado.predict(X_nuevos)\n",
    "\n",
    "print(\"Predicciones:\", predicciones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07722d7",
   "metadata": {},
   "source": [
    "# ¬øPara qu√© sirve `joblib`?\n",
    "\n",
    "`joblib` es una biblioteca de Python muy usada para:\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Guardar y cargar objetos grandes y complejos de forma eficiente\n",
    "\n",
    "Especialmente √∫til cuando trabaj√°s con:\n",
    "\n",
    "- Modelos entrenados (`scikit-learn`, `XGBoost`, etc.)\n",
    "- `Pipeline`s (escaladores + modelos)\n",
    "- Objetos de NumPy grandes (como matrices o vectores)\n",
    "- Listas, diccionarios, funciones, clases, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ ¬øPara qu√© se usa normalmente?\n",
    "\n",
    "### 1. Guardar modelos entrenados\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "joblib.dump(modelo_entrenado, 'modelo.pkl')\n",
    "```\n",
    "\n",
    "### 2. Cargar modelos ya entrenados\n",
    "\n",
    "```python\n",
    "modelo = joblib.load('modelo.pkl')\n",
    "```\n",
    "\n",
    "### üÜö ¬øPor qu√© no pickle?\n",
    "* joblib est√° optimizado para trabajar con arrays grandes de NumPy, lo cual es muy com√∫n en machine learning.\n",
    "\n",
    "* Es m√°s r√°pido y eficiente que pickle para objetos como modelos de scikit-learn.\n",
    "\n",
    "üîß Internamente, joblib usa compresi√≥n y almacenamiento binario eficiente.\n",
    "\n",
    "### üß† ¬øCu√°ndo usar joblib?\n",
    "\n",
    "| Situaci√≥n                                | ¬øUsar `joblib`?                |\n",
    "| ---------------------------------------- | ------------------------------ |\n",
    "| Guardar modelo de ML                     | ‚úÖ S√≠                           |\n",
    "| Guardar grandes arrays de NumPy          | ‚úÖ S√≠                           |\n",
    "| Guardar objetos comunes (texto, n√∫meros) | ‚ùå Mejor usar `pickle` o `json` |\n",
    "| Reutilizar pipelines o transformadores   | ‚úÖ S√≠                           |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
